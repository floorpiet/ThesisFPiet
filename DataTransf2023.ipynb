{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereby a \"checklist\" for what needs to be done before it can be put into a model:<br><br>\n",
    "- remove outliers <br>\n",
    "- create lags of relevant fields (product-specific demand is specifically interesting)<br>\n",
    "- make a variable that counts the number of weeks from the start of the set (as a numerical inflation variable)<br>\n",
    "- remove zero demand observations (after creating lags -> ptherwise lags will not be accurate. zero demand in lag will be handled differently *)<br>\n",
    "- log-transform all the numerical variables (after removing zero demand observations --> otherwise they become -infinity)<br>\n",
    "- replace the lagged variables that were zero and are now log-transformed to value -infinity (*) with a mean (or max in case the zero demand is due to promo) value of demand<br>\n",
    "- create dummy (0/1) variables out of the categorical features using a OneHotEncoder or pd.get_dummies (and leave 1 default dummy per categorical variable out: if you include all dummies you cannot work with it due to rank deficiency)<br>\n",
    "- include a constant<br><bar>\n",
    "So then all the relevant variables are ready and transformed; and it's important that all input variables are numerical (either a log-transformed value or a dummy variable) and there are no -infinity values left. <br>\n",
    "<br>\n",
    "Note: NaN values are okay for XGBoost models - but not for others such as Random Forest<br>\n",
    "potentially: you can include nonlinear terms such as squared values of variables (f.e. the aforementioned inflation variable, unavailability percentage, anything that makes sense in your opinion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from linearmodels import PanelOLS\n",
    "from linearmodels import RandomEffects\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import sklearn\n",
    "import statsmodels.api \n",
    "import seaborn as sns\n",
    "from dateutil import parser\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 4000\n",
    "pd.options.display.max_columns = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #in case after transformations PPL is still wrong:\n",
    "# df_log=pd.read_csv(\"dataframe_transformed_4_log.csv\")\n",
    "# new_col = pd.Series(['PPL1', 'PPL2', 'PPL3'] * (df_log.shape[0] // 3 + 1))[:df_log.shape[0]]\n",
    "# df_log['PPL']=new_col\n",
    "# df_log = pd.get_dummies(df_log, columns=['PPL'])\n",
    "# df_log['postShift'] = df_log['WEEK_START_DATE'] > \"2021-06-20\"\n",
    "# df_log['postPPL1'] = df_log['postShift']*df_log['PPL_PPL1']\n",
    "# df_log['postPPL2'] = df_log['postShift']*df_log['PPL_PPL2']\n",
    "# df_log['postPPL3'] = df_log['postShift']*df_log['PPL_PPL3']\n",
    "# df_log['PPL1'] = df_log['PPL_PPL1']-df_log['postPPL1']\n",
    "# df_log['PPL2'] = df_log['PPL_PPL2']-df_log['postPPL2']\n",
    "# df_log['PPL3'] = df_log['PPL_PPL3']-df_log['postPPL3']\n",
    "# df_log.to_csv('dataframe_transformed_5_log.csv') # opnieuw opslaan - het klopte niet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"DF_WEEK_DRINKS_2023.csv\")\n",
    "# df=df.drop('#',axis=1)\n",
    "df=df.set_index(['KEY_ARTICLE', 'KEY_WEEK','PPL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nans with zero\n",
    "df[['product order amt','AVG unavailability_perc']]=df[['product order amt','AVG unavailability_perc']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to make lagged variables (can also be done in SQL query)\n",
    "def buildLaggedFeatures(s,lag=2,dropna=False):\n",
    "\n",
    "# Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "\n",
    "\n",
    "    if type(s) is pd.Series:\n",
    "        the_range=range(lag+1)\n",
    "        res=pd.concat([s.groupby(['KEY_ARTICLE','PPL']).shift(i) for i in the_range],axis=1)\n",
    "        res.columns=['lag_%d' %i for i in the_range]\n",
    "    # elif type(s) is pd.DataFrame:\n",
    "    #     new_dict={}\n",
    "    #     for col_name in s:\n",
    "    #         new_dict[col_name]=s[col_name]\n",
    "    #         # create lagged Series\n",
    "    #         for l in range(1,lag+1):\n",
    "    #             new_dict['%s_lag%d' %(col_name,l)]=s[col_name].groupby(['KEY_ARTICLE','PPL']).shift(l) \n",
    "    #     res=pd.DataFrame(new_dict,index=s.index)\n",
    "    else:\n",
    "        print('Only works for (DataFrame or) Series')\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lags\n",
    "df_orderamt=buildLaggedFeatures(df['product order amt'],lag=2,dropna=False)\n",
    "df_orderamt.set_axis(['product order amt', 'product order amt_t-1', 'product order amt_t-2'], axis=1, inplace=True)\n",
    "df_price=buildLaggedFeatures(df['Avg sell price'],lag=2,dropna=False)\n",
    "df_price.set_axis(['Avg sell price', 'Avg sell price_t-1', 'Avg sell price_t-2'], axis=1, inplace=True)\n",
    "\n",
    "df[['product order amt_t-1', 'product order amt_t-2']] = df_orderamt[['product order amt_t-1', 'product order amt_t-2']]\n",
    "df[[ 'Avg sell price_t-1', 'Avg sell price_t-2']]=df_price[['Avg sell price_t-1', 'Avg sell price_t-2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose columns to include in dataframe\n",
    "vars= df[['product order amt', 'Avg sell price',\n",
    "       'Avg purchase price', \n",
    "        'AVG unavailability_perc',\n",
    "       'AVG_HIGH_TEMP', 'total order amt', \n",
    "       'NR_ARTICLES_IN_CAT','NR_ARTICLES_IN_CAT_2',\n",
    "       'product order amt_t-1', 'product order amt_t-2', 'Avg sell price_t-1',\n",
    "       'Avg sell price_t-2', 'YEAR_CALENDAR_WEEK', 'WEEK_START_DATE','ARTICLE_NAME','PROMO_DUMMY', 'ARTICLE_TIER', 'ART_BRAND_TIER', 'Packaging', 'ARTICLE_CAT_2', 'ARTICLE_CAT_3',\n",
    "       'ARTICLE_CAT_4', 'ARTICLE_ID','ART_CONTENT_VOLUME',\n",
    "       'ART_IS_MULTIPACK',]] \n",
    "# exclude cat3 (4)  (out of scope for now)\n",
    "# because Dummies cat3 (4) hade to be made within each cat2 (3): because per category level, for each ``group'' of subcategories that form a category: you need to remove one of \n",
    "# the subcategories for regression to be executed (rank deficiency issues occur in case all categories are included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after creating lags: remove first 2 weeks\n",
    "# make dataset start from week 53 on\n",
    "vars = vars[vars['product order amt_t-2'].notna()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default dummy values are:  {'PPL_postPPL1', 'ARTICLE_TIER_Best', 'ART_BRAND_TIER_A-brand', 'ARTICLE_CAT_2_Cola', 'ARTICLE_CAT_4_Appelsap', 'Packaging_Bottle', 'ARTICLE_CAT_3_Appelsap'}\n"
     ]
    }
   ],
   "source": [
    "#make dummies \n",
    "\n",
    "vars.reset_index('PPL', inplace=True) # you want PPL in dummified form\n",
    "dum_cols=['PPL','ARTICLE_TIER', 'ART_BRAND_TIER', 'Packaging', 'ARTICLE_CAT_2', 'ARTICLE_CAT_3', 'ARTICLE_CAT_4']\n",
    "print('Default dummy values are: ',(set(pd.get_dummies(vars,columns=dum_cols))).difference(set(pd.get_dummies(vars,columns=dum_cols,drop_first=True))))\n",
    "vars=pd.get_dummies(vars.join(vars0['ARTICLE_CAT_2'],rsuffix='_complete'),columns=dum_cols,drop_first=True)\n",
    "# vars=sm.tools.tools.add_constant(vars)\n",
    "df=vars.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(['KEY_ARTICLE', 'KEY_WEEK'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make week count number variable (inflatie / time measure)\n",
    "df.WEEK_START_DATE = df.WEEK_START_DATE.apply(lambda x: parser.parse(x))\n",
    "date_string = \"2020-12-28\"\n",
    "startDate = datetime.strptime(date_string, '%Y-%m-%d')\n",
    "df['DAYS_SINCE_START'] = df.WEEK_START_DATE.apply(lambda x: x - startDate)\n",
    "df['WEEKS_SINCE_START'] = df.DAYS_SINCE_START.apply(lambda x: x.days/7)\n",
    "\n",
    "df['WEEK_NR'] = df.WEEKS_SINCE_START.apply(lambda x: x + 1)\n",
    "df['AVG unavailability_perc']=df['AVG unavailability_perc'].apply(lambda x: x*100)\n",
    "df[['product order amt','AVG unavailability_perc']]=df[['product order amt','AVG unavailability_perc']].fillna(0)\n",
    "df[['KEY_ARTICLE', 'KEY_WEEK', 'YEAR_CALENDAR_WEEK','ARTICLE_NAME','ARTICLE_ID']]=df[['KEY_ARTICLE', 'KEY_WEEK', 'YEAR_CALENDAR_WEEK','ARTICLE_NAME','ARTICLE_ID']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Florence Piet\\AppData\\Local\\Temp\\ipykernel_19088\\4175568398.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  logdf.rename(columns=lambda x: 'l_'+x, inplace=True)\n",
      "c:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# apply log-transformation on subset of columns: can only apply np.log() on numerical features \n",
    "\n",
    "logdf=df[['KEY_ARTICLE','KEY_WEEK','product order amt','Avg sell price', 'Avg purchase price', 'AVG_HIGH_TEMP', 'total order amt','NR_ARTICLES_IN_CAT',\n",
    "'NR_ARTICLES_IN_CAT_2', 'product order amt_t-1',\n",
    "'product order amt_t-2', 'Avg sell price_t-1','Avg sell price_t-2', 'WEEK_NR','AVG unavailability_perc',\n",
    "]]\n",
    "logdf.set_index(['KEY_ARTICLE','KEY_WEEK'],inplace=True) # used to join with df\n",
    "logdf.rename(columns=lambda x: 'l_'+x, inplace=True)\n",
    "logdf=np.log(logdf)\n",
    "df_log=pd.concat([df,logdf],axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include a constant\n",
    "df_log=statsmodels.tools.tools.add_constant(df_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save transformed set\n",
    "df_log.to_csv('dataframe_transformed_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making polynomial features to include nonlinearity in linear models like Lasso\n",
    "\n",
    "# Set up set with polynomials\n",
    "data=df_log2.copy()\n",
    "data=data.reset_index()\n",
    "features = data.copy()[['l_Avg sell price','l_Avg purchase price','l_AVG_HIGH_TEMP', 'l_total order amt','l_NR_ARTICLES_IN_CAT', 'l_NR_ARTICLES_IN_CAT_2',\n",
    "# 'l_product order amt_t-1','l_product order amt_t-2', 'l_Avg sell price_t-1','l_Avg sell price_t-2', \n",
    "'l_WEEK_NR', 'AVG unavailability_perc']]\n",
    "\n",
    "poly_dict = {'l_Avg sell price':2,\n",
    "'l_Avg purchase price':2,\n",
    "'l_AVG_HIGH_TEMP':2,\n",
    "'l_total order amt':2,\n",
    "'l_NR_ARTICLES_IN_CAT':2,\n",
    "'l_NR_ARTICLES_IN_CAT_2':2,\n",
    "# 'l_product order amt_t-1':2,\n",
    "# 'l_product order amt_t-2':2,\n",
    "# 'l_Avg sell price_t-1':2,\n",
    "# 'l_Avg sell price_t-2':2,\n",
    "'l_WEEK_NR':2,\n",
    "'AVG unavailability_perc':2}\n",
    "\n",
    "for key, degree in poly_dict.items():\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    data_transf = poly.fit_transform(data[[key]])\n",
    "    x_cols = poly.get_feature_names_out([key])\n",
    "    data_transf = pd.DataFrame(data_transf, columns=x_cols)\n",
    "\n",
    "    features = pd.concat((features, data_transf),\n",
    "                          axis=1, sort=False,copy=False)\n",
    "\n",
    "df_log3 = pd.concat((data.copy()[['KEY_ARTICLE','KEY_WEEK','l_product order amt','const', \n",
    "      'l_product order amt_t-1','l_Avg sell price_t-1','l_product order amt_t-2','l_Avg sell price_t-2',\n",
    "       'PPL2', 'PPL3',\n",
    "       'YEAR_CALENDAR_WEEK', 'WEEK_START_DATE', 'ARTICLE_NAME', 'ARTICLE_ID',\n",
    "       'ARTICLE_TIER_Better', 'ARTICLE_TIER_Good',\n",
    "       'ART_BRAND_TIER_Price entry', 'ART_BRAND_TIER_Private label',\n",
    "       'Packaging_Box', 'Packaging_Can', 'Packaging_Pack','ARTICLE_CAT_2_complete',\n",
    "       'ARTICLE_CAT_2_Drinkpakjes', 'ARTICLE_CAT_2_Fruitdrank',\n",
    "       'ARTICLE_CAT_2_IJsthee', 'ARTICLE_CAT_2_Limonade & siropen',\n",
    "       'ARTICLE_CAT_2_Sappen & smoothies',\n",
    "       'ARTICLE_CAT_2_Sinas, Lemon & Cassis', 'ARTICLE_CAT_2_Speciaal fris',\n",
    "       'ARTICLE_CAT_2_Sport- & energydrink', 'ARTICLE_CAT_2_Water',\n",
    "       'PROMO_DUMMY', 'PPL1', 'postPPL1', 'postPPL2', 'postPPL3']], features.iloc[:,8:].copy()[['l_Avg sell price',\n",
    "       'l_Avg sell price^2', 'l_Avg purchase price', 'l_Avg purchase price^2',\n",
    "       'l_AVG_HIGH_TEMP', 'l_AVG_HIGH_TEMP^2', 'l_total order amt',\n",
    "       'l_total order amt^2',\n",
    "    #    'l_total order amt_t-1',\n",
    "    #    'l_total order amt_t-1^2', 'l_total order amt_t-2',\n",
    "    #    'l_total order amt_t-2^2',\n",
    "       'l_NR_ARTICLES_IN_CAT', 'l_NR_ARTICLES_IN_CAT^2',\n",
    "       'l_NR_ARTICLES_IN_CAT_2', 'l_NR_ARTICLES_IN_CAT_2^2', 'l_WEEK_NR',\n",
    "       'l_WEEK_NR^2', 'AVG unavailability_perc', 'AVG unavailability_perc^2']]),axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding holiday dummy variable\n",
    "listofHolidays=[\"2020-12-28\",\n",
    "                \"2021-01-04\",\n",
    "                \"2021-03-29\",\n",
    "                \"2021-05-03\",\n",
    "                \"2021-07-26\",\n",
    "                \"2021-08-02\",\n",
    "                \"2021-08-09\",\n",
    "                \"2021-08-16\",\n",
    "                \"2021-12-27\",\n",
    "                \"2022-01-03\",\n",
    "                \"2022-04-11\",\n",
    "                \"2022-05-02\",\n",
    "                \"2022-07-25\",\n",
    "                \"2022-08-01\",\n",
    "                \"2022-08-08\",\n",
    "                \"2022-08-15\",\n",
    "                \"2022-12-26\"\n",
    "                ]\n",
    "df_log[\"HOLIDAY\"]=(df_log['WEEK_START_DATE'].isin(listofHolidays))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping outliers\n",
    "df=df[~(df['WEEK_START_DATE'].isin([\"2021-09-06\", \"2021-12-13\"]))] # find date of outliers bij making a boxplot (in f.e. tableau: then you can easily spot the date of the outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a288de8946f0e8975c296c3ff94d685fc9d04b5938dbefc020b3262d6c1314ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
